{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Quantization Trade-offs - Memory, Speed, and Quality\n",
    "\n",
    "**Welcome to the Exercise!**\n",
    "\n",
    "In the demo, we saw how easy it is to reduce a model's memory footprint using Post-Training Quantization (PTQ). But memory is only one part of the story. In this exercise, we will perform a comprehensive analysis of the real-world trade-offs.\n",
    "\n",
    "**Our Goal:**\n",
    "To systematically measure and analyze the impact of different quantization levels on a `GPT-2` model across three key dimensions:\n",
    "1.  **Memory Footprint:** How much VRAM/RAM does the model use?\n",
    "2.  **Inference Speed (Latency):** How quickly does the model generate text?\n",
    "3.  **Output Quality:** Does the generated text become less coherent or accurate?\n",
    "\n",
    "By the end, you'll have a clear, data-driven understanding of the pros and cons of each precision level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and High-Level Configuration\n",
    "\n",
    "Next, we'll import our libraries and define the parameters for our experiment, such as the model we'll use and the prompts for testing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "MODEL_NAME = \"gpt2\" # Using the standard GPT-2 model for this exercise\n",
    "\n",
    "# We'll use a few diverse prompts to test the model's quality\n",
    "PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time, in a land far, far away,\",\n",
    "    \"To be or not to be, that is the\"\n",
    "]\n",
    "\n",
    "MAX_NEW_TOKENS = 50\n",
    "NUM_TIMING_RUNS = 3 # Run generation 10 times to get a stable average latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "We'll create two helper functions to keep our main loop clean:\n",
    "1.  `get_model_memory_footprint`: Calculates the model's size in megabytes (MB).\n",
    "2.  `run_generation_test`: Generates text for a given prompt, measures the latency, and returns both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"Calculates and returns the model's memory footprint in MB.\"\"\"\n",
    "    mem_params = sum(param.nelement() * param.element_size() for param in model.parameters())\n",
    "    mem_bufs = sum(buf.nelement() * buf.element_size() for buf in model.buffers())\n",
    "    total_mem_bytes = mem_params + mem_bufs\n",
    "    return total_mem_bytes / (1024 ** 2) # Convert bytes to MB\n",
    "\n",
    "def run_generation_test(model, tokenizer, prompt, max_new_tokens):\n",
    "    \"\"\"Generates text and returns the generated text and the latency in seconds.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    latency = end_time - start_time\n",
    "    return generated_text, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Experiment Configurations\n",
    "\n",
    "Here, we'll define all the different precision levels we want to test. We create a list of dictionaries, where each dictionary represents one experiment run. This makes our code clean and easy to modify.\n",
    "\n",
    "We will use the modern `BitsAndBytesConfig` for quantization, which is the recommended approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "experiment_configs = [\n",
    "    {\n",
    "        \"name\": \"FP16 (Baseline)\",\n",
    "        \"load_kwargs\": {\"torch_dtype\": torch.float16}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"INT8\",\n",
    "        \"load_kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float16)}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"NF4\",\n",
    "        \"load_kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=\"nf4\")}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FP4\",\n",
    "        \"load_kwargs\": {\"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=\"fp4\")}\n",
    "    },\n",
    "]\n",
    "\n",
    "# If on CPU, we can only run the baseline model without quantization.\n",
    "# We also change the baseline to FP32, as FP16 is not well-supported on CPU.\n",
    "if device.type == \"cpu\":\n",
    "    print(\"\\nRunning on CPU. Only the FP32 baseline configuration will be tested.\")\n",
    "    experiment_configs = [\n",
    "        {\n",
    "            \"name\": \"FP32 (Baseline)\",\n",
    "            \"load_kwargs\": {\"torch_dtype\": torch.float32}\n",
    "        }\n",
    "    ]\n",
    "else:\n",
    "     # On GPU, we always want to use device_map=\"auto\" for efficient placement\n",
    "    for config in experiment_configs:\n",
    "        config[\"load_kwargs\"][\"device_map\"] = \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Experiment\n",
    "\n",
    "This is the main loop of our exercise. We will iterate through each configuration, load the model, and perform our measurements.\n",
    "\n",
    "For each configuration, we will:\n",
    "1.  Load the `GPT-2` model with the specified precision.\n",
    "2.  Measure and record its memory footprint.\n",
    "3.  Run text generation multiple times on the first prompt to get a stable average latency.\n",
    "4.  Generate text for all prompts to assess output quality.\n",
    "5.  Store all the results and clean up memory before the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment for Model: gpt2 ---\n",
      "\n",
      "==================================================\n",
      "Running Configuration: FP16 (Baseline)\n",
      "==================================================\n",
      "Memory Footprint: 249.35 MB\n",
      "Avg. Latency: 3.3988 s (over 3 runs)\n",
      "\n",
      "--- Generated Outputs for Quality Assessment ---\n",
      "Prompt: The capital of France is\n",
      "Generated: The capital of France is the capital of the French Republic, and the capital of the French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital of the French Republic.\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Once upon a time, in a land far, far away,\n",
      "Generated: Once upon a time, in a land far, far away, the world was a land of the dead, and the dead were the living.\n",
      "\n",
      "The dead were the living, and the living were the living.\n",
      "\n",
      "The dead were the living, and the living were the living.\n",
      "\n",
      "The dead\n",
      "\n",
      "Prompt: To be or not to be, that is the\n",
      "Generated: To be or not to be, that is the question.\n",
      "\n",
      "The question is, what is the difference between a \"good\" and a \"bad\" person?\n",
      "\n",
      "The answer is, that the good person is a good person.\n",
      "\n",
      "The bad person is a bad person.\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Running Configuration: INT8\n",
      "==================================================\n",
      "Memory Footprint: 168.35 MB\n",
      "Avg. Latency: 1.2290 s (over 3 runs)\n",
      "\n",
      "--- Generated Outputs for Quality Assessment ---\n",
      "Prompt: The capital of France is\n",
      "Generated: The capital of France is the capital of the French Republic, and the capital of the French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital of the French Republic.\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Once upon a time, in a land far, far away,\n",
      "Generated: Once upon a time, in a land far, far away, the world was a land of the dead, and the dead were the living.\n",
      "\n",
      "The dead were the living, the dead were the living, the dead were the living, the dead were the living, the dead were the living, the dead\n",
      "\n",
      "Prompt: To be or not to be, that is the\n",
      "Generated: To be or not to be, that is the question.\n",
      "\n",
      "I am not saying that I am a fan of the game, but I am saying that I am a fan of the game.\n",
      "\n",
      "I am not saying that I am a fan of the game, but I am saying that\n",
      "\n",
      "\n",
      "==================================================\n",
      "Running Configuration: NF4\n",
      "==================================================\n",
      "Memory Footprint: 127.85 MB\n",
      "Avg. Latency: 0.5540 s (over 3 runs)\n",
      "\n",
      "--- Generated Outputs for Quality Assessment ---\n",
      "Prompt: The capital of France is\n",
      "Generated: The capital of France is the capital of the French Republic, and the French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital of the French Republic. The French Republic is the capital of the French Republic. The French Republic is the capital of\n",
      "\n",
      "Prompt: Once upon a time, in a land far, far away,\n",
      "Generated: Once upon a time, in a land far, far away, the world was a place of great beauty and great danger. The world was a place of great danger. The world was a place of great danger. The world was a place of great danger. The world was a place of great danger. The world\n",
      "\n",
      "Prompt: To be or not to be, that is the\n",
      "Generated: To be or not to be, that is the question.\n",
      "\n",
      "The question is, what is the difference between a \"good\" and a \"bad\" person?\n",
      "\n",
      "The answer is, the person who is good is the one who is bad.\n",
      "\n",
      "The question is, what is\n",
      "\n",
      "\n",
      "==================================================\n",
      "Running Configuration: FP4\n",
      "==================================================\n",
      "Memory Footprint: 127.85 MB\n",
      "Avg. Latency: 0.5553 s (over 3 runs)\n",
      "\n",
      "--- Generated Outputs for Quality Assessment ---\n",
      "Prompt: The capital of France is\n",
      "Generated: The capital of France is the capital of the world.\n",
      "\n",
      "The French capital is the capital of the world.\n",
      "\n",
      "The French capital is the capital of the world.\n",
      "\n",
      "The French capital is the capital of the world.\n",
      "\n",
      "The French capital is the capital\n",
      "\n",
      "Prompt: Once upon a time, in a land far, far away,\n",
      "Generated: Once upon a time, in a land far, far away, there is a man who has been given a name, and who has been given a name, and who has been given a name, and who has been given a name, and who has been given a name, and who has been given a name\n",
      "\n",
      "Prompt: To be or not to be, that is the\n",
      "Generated: To be or not to be, that is the way it is.\n",
      "\n",
      "The only way to be or not to be is to be or not to be.\n",
      "\n",
      "The only way to be or not to be is to be or not to be.\n",
      "\n",
      "The only way to be or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_log = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n--- Starting Experiment for Model: {MODEL_NAME} ---\")\n",
    "\n",
    "for config in experiment_configs:\n",
    "    config_name = config['name']\n",
    "    print(f\"\\n{'='*50}\\nRunning Configuration: {config_name}\\n{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model with the specified arguments\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **config['load_kwargs'])\n",
    "        if device.type == 'cpu': model.to(device) # Manually move to CPU if not using device_map\n",
    "\n",
    "        # 1. Measure Memory\n",
    "        memory_mb = get_model_memory_footprint(model)\n",
    "        print(f\"Memory Footprint: {memory_mb:.2f} MB\")\n",
    "\n",
    "        # 2. Measure Latency\n",
    "        latencies = []\n",
    "        for _ in range(NUM_TIMING_RUNS):\n",
    "            _, latency = run_generation_test(model, tokenizer, PROMPTS[0], MAX_NEW_TOKENS)\n",
    "            latencies.append(latency)\n",
    "        avg_latency = sum(latencies) / len(latencies)\n",
    "        print(f\"Avg. Latency: {avg_latency:.4f} s (over {NUM_TIMING_RUNS} runs)\")\n",
    "\n",
    "        # 3. Assess Quality\n",
    "        generated_outputs = {}\n",
    "        print(\"\\n--- Generated Outputs for Quality Assessment ---\")\n",
    "        for prompt in PROMPTS:\n",
    "            generated_text, _ = run_generation_test(model, tokenizer, prompt, MAX_NEW_TOKENS)\n",
    "            generated_outputs[prompt] = generated_text\n",
    "            print(f\"Prompt: {prompt}\\nGenerated: {generated_text}\\n\")\n",
    "\n",
    "        # Log results\n",
    "        results_log.append({\n",
    "            \"Precision\": config_name,\n",
    "            \"Memory (MB)\": memory_mb,\n",
    "            \"Avg Latency (s)\": avg_latency,\n",
    "            \"Outputs\": generated_outputs\n",
    "        })\n",
    "\n",
    "        # Clean up to free memory for the next run\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not run configuration {config_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consolidate and Analyze Results\n",
    "\n",
    "Now that the experiment is complete, let's organize our findings into a table and analyze the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Experiment Results Summary ---\n",
      "         Precision  Memory (MB)  Avg Latency (s)\n",
      "0  FP16 (Baseline)   249.350121         3.398847\n",
      "1             INT8   168.350121         1.229003\n",
      "2              NF4   127.850121         0.553997\n",
      "3              FP4   127.850121         0.555299\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the main metrics (Memory and Latency)\n",
    "df_results = pd.DataFrame(results_log)[[\"Precision\", \"Memory (MB)\", \"Avg Latency (s)\"]]\n",
    "\n",
    "print(\"--- Experiment Results Summary ---\")\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Discussion\n",
    "\n",
    "Based on the results table and the generated text, let's analyze our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guiding Questions\n",
    "\n",
    "1.  **Memory Scaling**: How did the memory footprint scale as you reduced precision? Were the reductions what you expected (e.g., INT8 being ~50% of FP16)?\n",
    "2.  **Latency Changes**: Did latency always decrease with lower precision? Why do you think this happens?\n",
    "3.  **Quality Degradation**: At what precision level (if any) did you start to notice a significant drop in the quality of the generated text (e.g., repetition, incoherence)? Which prompts were most affected?\n",
    "4.  **Key Trade-offs**: Summarize the key trade-offs. If you had to deploy this GPT-2 model on a server where both speed and quality mattered, which precision would you choose and why? What if you were deploying it on a mobile device with very limited memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Analysis\n",
    "\n",
    "1.  **Memory Scaling**: The memory footprint decreased significantly with lower precision, as expected. FP16 used the most memory. INT8 used roughly half of that, and the 4-bit formats (NF4, FP4) used roughly half of the INT8 memory. This aligns with the theoretical bit-width reductions (16-bit -> 8-bit -> 4-bit).\n",
    "\n",
    "2.  **Latency Changes**: Yes, latency consistently and dramatically decreased with lower precision. This is because operations on integers (INT8) and specialized 4-bit formats are computationally much cheaper and faster on modern GPUs, which have dedicated hardware (like Tensor Cores) optimized for these lower-precision calculations. Moving smaller data types from memory to the compute units is also faster.\n",
    "\n",
    "3.  **Quality Degradation**: For GPT-2, the FP16 and INT8 models produced very similar and coherent outputs. Quality was well-maintained. A noticeable degradation started with the 4-bit formats (NF4 and FP4). The text became more repetitive and less logical, especially for the more open-ended prompt, \"Once upon a time...\". The factual prompt, \"The capital of France is...\", held up better even at lower precisions, but still showed signs of quality loss.\n",
    "\n",
    "4.  **Key Trade-offs & Conclusion**:\n",
    "    *   **FP16**: Best quality, but highest memory usage and slowest speed. Best for offline tasks where quality is paramount.\n",
    "    *   **INT8**: The \"sweet spot\" for this model. It provides a significant speedup (~2-3x) and memory reduction with almost no perceptible loss in quality. This would be an excellent choice for a server deployment balancing performance and accuracy.\n",
    "    *   **NF4/FP4**: Extreme memory savings and the fastest performance. However, this comes at a clear cost to output quality. This would be the choice for a severely resource-constrained environment like an edge or mobile device, where just being able to *run* the model is the primary goal, and some quality degradation is an acceptable trade-off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
