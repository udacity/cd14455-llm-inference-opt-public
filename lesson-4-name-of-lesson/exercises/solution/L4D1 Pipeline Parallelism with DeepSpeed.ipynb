{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Pipeline Parallelism with DeepSpeed - The \"Happy Path\"\n",
    "\n",
    "The purpose of this demo is to provide a clear, simple, and successful \"first look\" at pipeline parallelism. We will demonstrate:\n",
    "1.  How a simple, sequential model can be **automatically partitioned** by DeepSpeed across multiple GPUs.\n",
    "2.  The basic **mechanics** of launching a DeepSpeed job using a configuration file.\n",
    "3.  The **outcome**: a single model running collaboratively on multiple devices.\n",
    "\n",
    "This demo represents the ideal, \"happy path\" scenario. The model we use is structured perfectly for DeepSpeed's automatic partitioning, so we expect it to work right out of the box.\n",
    "\n",
    "### How We'll Run This in a Notebook\n",
    "The `deepspeed` command is an external launcher. To make this work seamlessly in a notebook, we will:\n",
    "1.  **Programmatically create** a JSON configuration file.\n",
    "2.  **Write our Python logic to a script** using the `%%writefile` magic command.\n",
    "3.  **Execute the `deepspeed` launcher** on that script directly from the notebook using `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install the necessary libraries and then import them, checking to ensure our environment is set up correctly. This demo requires at least 4 GPUs to see pipeline parallelism in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vqyk2A989Q22",
    "outputId": "53fecd0c-66b9-4caf-e955-dc1ce342ea7b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting deepspeed\n",
      "  Downloading deepspeed-0.17.2.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Collecting einops (from deepspeed)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepspeed) (6.1.1)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepspeed) (2.9.2)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepspeed) (2.2.2)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepspeed) (12.575.51)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (2.23.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->deepspeed) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m194.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m179.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: rouge_score, deepspeed\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=cf803f766fdc27f7bc71cbffdcb9558b91efa27449337e94551815446182d74e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "\u001b[33m  DEPRECATION: Building 'deepspeed' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'deepspeed'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.17.2-py3-none-any.whl size=1699971 sha256=b9908db087f614703c1b7be5e3aabcaca335f4ad623537bf24faf7b4c2cca28c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/aa/90/50/536155ec6a2eb601f3052cfe5cab26cbb88f4a012c99566400\n",
      "Successfully built rouge_score deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, xxhash, safetensors, propcache, ninja, multidict, msgpack, hf-xet, fsspec, frozenlist, einops, dill, async-timeout, aiohappyeyeballs, absl-py, yarl, rouge_score, multiprocess, huggingface_hub, aiosignal, tokenizers, deepspeed, aiohttp, transformers, datasets, evaluate\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 8/27\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 8/27\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 8/27\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 8/27\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: dill0m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 9/27\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m 9/27\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m 9/27\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.090m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12/27\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: multiprocess[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12/27\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m12/27\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/27\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m18/27\u001b[0m [multiprocess]\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m27/27\u001b[0m [evaluate]/27\u001b[0m [evaluate]ers]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 deepspeed-0.17.2 dill-0.3.8 einops-0.8.1 evaluate-0.4.5 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.5 hjson-3.1.0 huggingface_hub-0.33.4 msgpack-1.1.1 multidict-6.6.3 multiprocess-0.70.16 ninja-1.11.1.4 propcache-0.3.2 py-cpuinfo-9.0.0 rouge_score-0.1.2 safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets rouge_score deepspeed transformers evaluate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5r_5OP749Is9",
    "outputId": "818c9200-c76d-4559-c7f6-5a40d594390b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 04:08:19,216] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 04:08:33,980] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "PyTorch version: 2.2.2\n",
      "DeepSpeed version: 0.17.2\n",
      "CUDA is available: True\n",
      "Number of GPUs available: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() < 4:\n",
    "        print(\"!! WARNING: This demo is designed for 4 GPUs. It may not run correctly. !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a \"Pipeline-Friendly\" Model\n",
    "\n",
    "Next, we'll create a model that is perfectly suited for pipeline parallelism. It is a simple `nn.Sequential` model.\n",
    "\n",
    "DeepSpeed's `\"uniform\"` partition method can easily inspect this sequence of layers and divide it into `N` even chunks for `N` GPUs. This makes the setup extremely simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2m9XZ9mu9Ko5",
    "outputId": "202bd6bc-28cd-4526-d25f-3a01a16036f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure for the Demo:\n",
      "SequentialDemoModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (15): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "This nn.Sequential structure is ideal for DeepSpeed's automatic partitioning.\n"
     ]
    }
   ],
   "source": [
    "# We will write this model definition directly into our script in the next step.\n",
    "# Here is the code for inspection:\n",
    "\n",
    "class SequentialDemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_layers=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # A simple list of layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Wrapping layers in `nn.Sequential` is the key to making partitioning easy\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Let's inspect the model structure\n",
    "model_for_inspection = SequentialDemoModel()\n",
    "print(\"Model Structure for the Demo:\")\n",
    "print(model_for_inspection)\n",
    "print(\"\\nThis nn.Sequential structure is ideal for DeepSpeed's automatic partitioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the DeepSpeed Configuration\n",
    "\n",
    "This JSON configuration file is the control panel for DeepSpeed. It tells the launcher how to set up our job. The most important part for this demo is the `\"pipeline\"` section, where we define how the model should be split.\n",
    "\n",
    "- `stages`: The number of pipeline stages to split the model into. This should match the number of GPUs we use.\n",
    "- `partition_method`: How to partition the layers. `\"uniform\"` splits the layer sequence as evenly as possible.\n",
    "- `micro_batch_size`: The size of the smaller data chunks that flow through the pipeline to keep all GPUs busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaOjlMha-QH9",
    "outputId": "aa5dbfc3-b61d-4933-d8d6-d1696f687f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed configuration file 'ds_config_demo.json' created.\n",
      "\n",
      "--- Demo Config Contents ---\n",
      "{\n",
      "  \"train_batch_size\": 16,\n",
      "  \"optimizer\": {\n",
      "    \"type\": \"SGD\",\n",
      "    \"params\": {\n",
      "      \"lr\": 0.001\n",
      "    }\n",
      "  },\n",
      "  \"pipeline\": {\n",
      "    \"stages\": 2,\n",
      "    \"partition_method\": \"uniform\",\n",
      "    \"micro_batch_size\": 8\n",
      "  },\n",
      "  \"comms_logger\": {\n",
      "    \"enabled\": false,\n",
      "    \"verbose\": false,\n",
      "    \"debug\": false\n",
      "  }\n",
      "}\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_config_demo = {\n",
    "  \"train_batch_size\": 16, # Global batch size\n",
    "\n",
    "  # A dummy optimizer is required by the DeepSpeed initializer\n",
    "  \"optimizer\": { \"type\": \"SGD\", \"params\": { \"lr\": 0.001 } },\n",
    "\n",
    "  # Pipeline Parallelism Configuration\n",
    "  \"pipeline\": {\n",
    "    \"stages\": 2,              # We will split the model into 2 stages for our 2 GPUs\n",
    "    \"partition_method\": \"uniform\", # Tells DeepSpeed to split layers evenly. Perfect for nn.Sequential.\n",
    "    \"micro_batch_size\": 8     # We split our global batch size into smaller micro-batches\n",
    "  },\n",
    "\n",
    "  \"comms_logger\": {\n",
    "    \"enabled\": False,\n",
    "    \"verbose\": False,\n",
    "    \"debug\": False\n",
    "  }\n",
    "}\n",
    "\n",
    "# Write the configuration to a file\n",
    "config_filename_demo = 'ds_config_demo.json'\n",
    "with open(config_filename_demo, 'w') as f:\n",
    "    json.dump(ds_config_demo, f, indent=2)\n",
    "\n",
    "print(f\"DeepSpeed configuration file '{config_filename_demo}' created.\")\n",
    "print(\"\\n--- Demo Config Contents ---\")\n",
    "print(json.dumps(ds_config_demo, indent=2))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write the Main Execution Script\n",
    "\n",
    "Now we'll package our logic into a Python script. The `deepspeed` launcher will execute this script on each GPU. \n",
    "\n",
    "Inside, the `deepspeed.initialize()` function is the key. \n",
    "\n",
    "It reads the configuration, performs the model partitioning, and returns a wrapped `model_engine` that handles all the complex distributed logic for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8O9-mxAJ-R3u",
    "outputId": "c55a8505-e755-4b16-91db-277a64f06a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_pipeline_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_pipeline_script.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import argparse\n",
    "\n",
    "# The model definition is included in the script\n",
    "class SequentialDemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Standard DeepSpeed argument parsing\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local rank\")\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 1. Instantiate the Model\n",
    "    # DeepSpeed will handle placing it on the correct device.\n",
    "    model = SequentialDemoModel()\n",
    "\n",
    "    # 2. Initialize with DeepSpeed\n",
    "    # This is the core function where the magic happens. DeepSpeed reads the\n",
    "    # config, partitions the model, and wraps it in a `model_engine`.\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        args=args,\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "    )\n",
    "\n",
    "    # 3. Print device information to verify partitioning\n",
    "    # Each rank (GPU) will print which device its partition is on.\n",
    "    print(\n",
    "        f\"Rank {model_engine.local_rank}: My model partition is on device: {model_engine.device}\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # 4. Run a forward pass\n",
    "    # We create input data and pass it to the engine. DeepSpeed handles\n",
    "    # passing the data through the pipeline stages automatically.\n",
    "    batch_size = 16\n",
    "    hidden_size = 1024\n",
    "    dummy_input = torch.randn(batch_size, hidden_size, device=model_engine.device)\n",
    "\n",
    "    output = model_engine(dummy_input)\n",
    "\n",
    "    # The final output is only available on the last stage of the pipeline.\n",
    "    # We can check this to confirm the run was successful.\n",
    "    if model_engine.is_last_stage():\n",
    "        print(f\"\\nRank {model_engine.local_rank} (Last Stage): Inference successful!\", flush=True)\n",
    "        print(f\"Final output shape: {output.shape}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the DeepSpeed Job!\n",
    "\n",
    "It's time to run our demo. We will execute the `deepspeed` launcher, telling it to use 4 GPUs. \n",
    "\n",
    "We'll also add some environment variables (`DS_LOG_LEVEL=ERROR` and `PYTHONWARNINGS=ignore`) to reduce log verbosity and keep the output clean.\n",
    "\n",
    "Pay close attention to the output logs. You should see a message from each rank confirming which GPU its model partition is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MI82e0E8-U0Q",
    "outputId": "6d7e52f2-44ba-42ad-ad46-83d254dc851c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泅 Launching DeepSpeed Pipeline Parallelism Demo...\n",
      "Rank 1: My model partition is on device: cuda:1\n",
      "Rank 2: My model partition is on device: cuda:2\n",
      "Rank 3: My model partition is on device: cuda:3\n",
      "Rank 0: My model partition is on device: cuda:0\n",
      "\n",
      "Rank 3 (Last Stage): Inference successful!\n",
      "Final output shape: torch.Size([16, 1024])\n",
      "\n",
      "[2025-07-14 04:53:48,894] [INFO] [launch.py:351:main] Process 25565 exits successfully.\n",
      "[2025-07-14 04:53:48,895] [INFO] [launch.py:351:main] Process 25564 exits successfully.\n",
      "[2025-07-14 04:53:48,895] [INFO] [launch.py:351:main] Process 25567 exits successfully.\n",
      "[2025-07-14 04:53:49,896] [INFO] [launch.py:351:main] Process 25566 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"泅 Launching DeepSpeed Pipeline Parallelism Demo...\")\n",
    "!DS_LOG_LEVEL=ERROR PYTHONWARNINGS=ignore deepspeed --num_gpus 4 demo_pipeline_script.py --deepspeed_config ds_config_demo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis and Conclusion\n",
    "\n",
    "You should have seen output similar to this:\n",
    "\n",
    "```\n",
    "Rank 0: My model partition is on device: cuda:0\n",
    "Rank 1: My model partition is on device: cuda:1\n",
    "...\n",
    "Rank 3 (Last Stage): Inference successful!\n",
    "Final output shape: torch.Size()\n",
    "```\n",
    "\n",
    "**Success!** This confirms that:\n",
    "- DeepSpeed successfully launched 4 processes, one for each GPU.\n",
    "- It partitioned our `SequentialDemoModel` and placed each partition on a separate GPU.\n",
    "- It correctly managed the data flow through the pipeline to produce a final result on the last stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijW_kYNe-WZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
